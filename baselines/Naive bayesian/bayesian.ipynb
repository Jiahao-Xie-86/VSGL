{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with seed: 3\n",
      "Training set class distribution: Counter({0: 400, 1: 400})\n",
      "Validation set class distribution: Counter({0: 100, 1: 100})\n",
      "Test set class distribution: Counter({1: 250, 0: 250})\n",
      "Validation Accuracy: 55.00%\n",
      "Validation F1 Score: 0.5946\n",
      "Test Accuracy: 57.40%\n",
      "Test F1 Score: 0.6283\n",
      "\n",
      "Running model with seed: 7\n",
      "Training set class distribution: Counter({1: 400, 0: 400})\n",
      "Validation set class distribution: Counter({1: 100, 0: 100})\n",
      "Test set class distribution: Counter({0: 250, 1: 250})\n",
      "Validation Accuracy: 59.50%\n",
      "Validation F1 Score: 0.6124\n",
      "Test Accuracy: 54.60%\n",
      "Test F1 Score: 0.5626\n",
      "\n",
      "Running model with seed: 11\n",
      "Training set class distribution: Counter({0: 400, 1: 400})\n",
      "Validation set class distribution: Counter({0: 100, 1: 100})\n",
      "Test set class distribution: Counter({0: 250, 1: 250})\n",
      "Validation Accuracy: 49.00%\n",
      "Validation F1 Score: 0.5565\n",
      "Test Accuracy: 59.40%\n",
      "Test F1 Score: 0.6530\n",
      "\n",
      "Running model with seed: 13\n",
      "Training set class distribution: Counter({1: 400, 0: 400})\n",
      "Validation set class distribution: Counter({0: 100, 1: 100})\n",
      "Test set class distribution: Counter({1: 250, 0: 250})\n",
      "Validation Accuracy: 58.50%\n",
      "Validation F1 Score: 0.5951\n",
      "Test Accuracy: 57.60%\n",
      "Test F1 Score: 0.5954\n",
      "\n",
      "Running model with seed: 23\n",
      "Training set class distribution: Counter({0: 400, 1: 400})\n",
      "Validation set class distribution: Counter({0: 100, 1: 100})\n",
      "Test set class distribution: Counter({1: 250, 0: 250})\n",
      "Validation Accuracy: 52.50%\n",
      "Validation F1 Score: 0.5498\n",
      "Test Accuracy: 55.80%\n",
      "Test F1 Score: 0.5758\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "import networkx as nx \n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "def set_random_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# ---- Step 1: Feature Extraction from Graphs ----\n",
    "def extract_graph_features(adj_matrix):\n",
    "    \"\"\"\n",
    "    Extracts features from a given adjacency matrix of a graph.\n",
    "    Features:\n",
    "    - Number of nodes\n",
    "    - Number of edges\n",
    "    - Graph density\n",
    "    - Average node degree\n",
    "    - Average clustering coefficient\n",
    "    \"\"\"\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()// 2\n",
    "    density = nx.density(G)\n",
    "    avg_degree = sum(dict(G.degree()).values()) / num_nodes\n",
    "\n",
    "    \n",
    "    return np.array([num_nodes,num_edges,density,avg_degree])\n",
    "\n",
    "# ---- Step 2: Load Dataset and Split into Training/Validation/Testing ----\n",
    "def load_and_extract_features(hamiltonian_dir, non_hamiltonian_dir):\n",
    "    # Hamiltonian graph files\n",
    "    hamiltonian_files = [os.path.join(hamiltonian_dir, f) for f in os.listdir(hamiltonian_dir) if f.endswith('.npy')]\n",
    "    # Non-Hamiltonian graph files\n",
    "    non_hamiltonian_files = [os.path.join(non_hamiltonian_dir, f) for f in os.listdir(non_hamiltonian_dir) if f.endswith('.npy')]\n",
    "    \n",
    "    # Extract features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in hamiltonian_files:\n",
    "        adj_matrix = np.load(file)\n",
    "        features.append(extract_graph_features(adj_matrix))\n",
    "        labels.append(1)  # Hamiltonian = 1\n",
    "    \n",
    "    for file in non_hamiltonian_files:\n",
    "        adj_matrix = np.load(file)\n",
    "        features.append(extract_graph_features(adj_matrix))\n",
    "        labels.append(0)  # Non-Hamiltonian = 0\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# ---- Step 3: Sample Training, Validation, and Testing Datasets ----\n",
    "def sample_train_val_test(features, labels, test_size=500, val_ratio=0.2, seed=42, sample_size=None):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and testing sets.\n",
    "    \n",
    "    Args:\n",
    "    - features: The extracted graph features.\n",
    "    - labels: The corresponding graph labels.\n",
    "    - test_size: The number of samples reserved for testing.\n",
    "    - val_ratio: The proportion of validation samples in the training/validation split.\n",
    "    - seed: Random seed for reproducibility.\n",
    "    - sample_size: The total number of samples for training and validation (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - X_train: Training set features.\n",
    "    - X_val: Validation set features.\n",
    "    - X_test: Test set features.\n",
    "    - y_train: Training set labels.\n",
    "    - y_val: Validation set labels.\n",
    "    - y_test: Test set labels.\n",
    "    \"\"\"\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    # Split off the test set (fixed size of 500)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(features, labels, test_size=test_size, random_state=seed, stratify=labels)\n",
    "    \n",
    "    # If sample_size is specified, sample that many from the remaining training+validation set\n",
    "    if sample_size is not None:\n",
    "        X_train_val, _, y_train_val, _ = train_test_split(X_train_val, y_train_val, train_size=sample_size, random_state=seed, stratify=y_train_val)\n",
    "    \n",
    "    # Split training+validation into 80% training and 20% validation\n",
    "    train_size = int((1 - val_ratio) * len(X_train_val))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size=train_size, random_state=seed, stratify=y_train_val)\n",
    "    \n",
    "    # ---- Print the class distribution of each dataset ----\n",
    "    print(f\"Training set class distribution: {Counter(y_train)}\")\n",
    "    print(f\"Validation set class distribution: {Counter(y_val)}\")\n",
    "    print(f\"Test set class distribution: {Counter(y_test)}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# ---- Step 4: Apply Naive Bayes Model ----\n",
    "def apply_naive_bayes(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train Naive Bayes Model\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = nb_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_test_pred = nb_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# ---- Main Workflow ----\n",
    "# hamiltonian_dir = './hamiltonian_small_mat'\n",
    "# non_hamiltonian_dir = './non_hamiltonian_small_mat'\n",
    "hamiltonian_dir = './small_Hamiltonian_hard'\n",
    "non_hamiltonian_dir = './small_non_Hamiltonian_hard'\n",
    "\n",
    "# Load and extract features from the dataset\n",
    "features, labels = load_and_extract_features(hamiltonian_dir, non_hamiltonian_dir)\n",
    "\n",
    "# Sample training, validation, and test sets (with sample size control)\n",
    "sample_size = 1000  # Experiment with different sample sizes\n",
    "\n",
    "# Define the seeds you want to use\n",
    "seeds = [3, 7, 11, 13, 23]\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\nRunning model with seed: {seed}\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = sample_train_val_test(features, labels, test_size=500, val_ratio=0.2, seed=seed, sample_size=sample_size)\n",
    "\n",
    "    # Apply Naive Bayes and evaluate\n",
    "    apply_naive_bayes(X_train, X_val, X_test, y_train, y_val, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphormer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
