{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import GraphormerForGraphClassification, GraphormerConfig\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # For deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, hamiltonian_dir, non_hamiltonian_dir):\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load Hamiltonian graphs (label = 1)\n",
    "        for filename in os.listdir(hamiltonian_dir):\n",
    "            if filename.endswith('.npy'):\n",
    "                adj_matrix = np.load(os.path.join(hamiltonian_dir, filename))\n",
    "                self.graphs.append(adj_matrix)\n",
    "                self.labels.append(1)\n",
    "\n",
    "        # Load Non-Hamiltonian graphs (label = 0)\n",
    "        for filename in os.listdir(non_hamiltonian_dir):\n",
    "            if filename.endswith('.npy'):\n",
    "                adj_matrix = np.load(os.path.join(non_hamiltonian_dir, filename))\n",
    "                self.graphs.append(adj_matrix)\n",
    "                self.labels.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        adj_matrix = self.graphs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            'adj_matrix': torch.tensor(adj_matrix, dtype=torch.float),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Function to compute metrics like accuracy and F1-score\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')  # Use 'macro' for multi-class problems\n",
    "    return accuracy, f1\n",
    "\n",
    "def collate_fn(batch):\n",
    "    adj_matrices = [item['adj_matrix'] for item in batch]\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "\n",
    "    max_size = max(adj.shape[0] for adj in adj_matrices)\n",
    "    num_heads = config.num_attention_heads\n",
    "    max_dist = 20\n",
    "\n",
    "    adj_padded, node_features, in_degree_list, out_degree_list, spatial_pos_list = [], [], [], [], []\n",
    "    attn_bias_list, attn_edge_type_list, input_edges_list = [], [], []\n",
    "\n",
    "    for adj in adj_matrices:\n",
    "        size = adj.shape[0]\n",
    "        pad_size = max_size - size\n",
    "\n",
    "        adj_pad = torch.nn.functional.pad(adj, (0, pad_size, 0, pad_size), \"constant\", 0)\n",
    "        adj_padded.append(adj_pad)\n",
    "\n",
    "        G = nx.from_numpy_array(adj.numpy())\n",
    "\n",
    "        degrees = np.array([degree for node, degree in G.degree()])\n",
    "        node_feat = torch.tensor(degrees, dtype=torch.long).unsqueeze(1)\n",
    "        node_feat_pad = torch.nn.functional.pad(node_feat, (0, 0, 0, pad_size), \"constant\", 0)\n",
    "        node_features.append(node_feat_pad)\n",
    "\n",
    "        in_degree_pad = np.pad(degrees, (0, pad_size), 'constant')\n",
    "        out_degree_pad = np.pad(degrees, (0, pad_size), 'constant')\n",
    "        in_degree_list.append(torch.tensor(in_degree_pad, dtype=torch.long))\n",
    "        out_degree_list.append(torch.tensor(out_degree_pad, dtype=torch.long))\n",
    "\n",
    "        spatial_pos = np.zeros((size, size), dtype=np.int64)\n",
    "        lengths = dict(nx.all_pairs_shortest_path_length(G, cutoff=max_dist))\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                spatial_pos[i, j] = lengths[i][j] if j in lengths[i] else max_dist\n",
    "        spatial_pos_pad = np.pad(spatial_pos, ((0, pad_size), (0, pad_size)), 'constant', constant_values=max_dist)\n",
    "        spatial_pos_list.append(torch.tensor(spatial_pos_pad, dtype=torch.long))\n",
    "\n",
    "        edge_type = adj.numpy().astype(np.int64)\n",
    "        edge_type_pad = np.pad(edge_type, ((0, pad_size), (0, pad_size)), 'constant', constant_values=0)\n",
    "        attn_edge_type_list.append(torch.tensor(edge_type_pad, dtype=torch.long))\n",
    "\n",
    "        # Construct input_edges based on shortest paths\n",
    "        input_edges = np.zeros((size, size, max_dist), dtype=np.int64)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                if j in lengths[i]:\n",
    "                    dist = lengths[i][j]\n",
    "                    if dist < max_dist:\n",
    "                        input_edges[i, j, dist] = 1\n",
    "        input_edges = np.repeat(input_edges[:, :, :, np.newaxis], num_heads, axis=3)\n",
    "        input_edges_pad = np.pad(input_edges, ((0, pad_size), (0, pad_size), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "        input_edges_list.append(torch.tensor(input_edges_pad, dtype=torch.long))\n",
    "\n",
    "        attn_bias = np.zeros((max_size + 1, max_size + 1), dtype=np.float32)\n",
    "        attn_bias_list.append(torch.tensor(attn_bias))\n",
    "\n",
    "    adj_padded = torch.stack(adj_padded)\n",
    "    node_features = torch.stack(node_features)\n",
    "    in_degree = torch.stack(in_degree_list)\n",
    "    out_degree = torch.stack(out_degree_list)\n",
    "    spatial_pos = torch.stack(spatial_pos_list)\n",
    "    attn_bias = torch.stack(attn_bias_list)\n",
    "    attn_edge_type = torch.stack(attn_edge_type_list)\n",
    "    input_edges = torch.stack(input_edges_list)\n",
    "\n",
    "    return {\n",
    "        'input_nodes': node_features,\n",
    "        'attn_bias': attn_bias,\n",
    "        'in_degree': in_degree,\n",
    "        'out_degree': out_degree,\n",
    "        'spatial_pos': spatial_pos,\n",
    "        'attn_edge_type': attn_edge_type,\n",
    "        'input_edges': input_edges,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_nodes = batch['input_nodes'].to(device)\n",
    "            attn_bias = batch['attn_bias'].to(device)\n",
    "            in_degree = batch['in_degree'].to(device)\n",
    "            out_degree = batch['out_degree'].to(device)\n",
    "            spatial_pos = batch['spatial_pos'].to(device)\n",
    "            attn_edge_type = batch['attn_edge_type'].to(device)\n",
    "            input_edges = batch['input_edges'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_nodes=input_nodes,\n",
    "                input_edges=input_edges,\n",
    "                attn_bias=attn_bias,\n",
    "                in_degree=in_degree,\n",
    "                out_degree=out_degree,\n",
    "                spatial_pos=spatial_pos,\n",
    "                attn_edge_type=attn_edge_type\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(torch.tensor(all_labels), torch.tensor(all_preds), average='macro')\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, f1\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, test_loader, device, optimizer, criterion, num_epochs, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    best_accuracy = 0  # Initialize best accuracy\n",
    "    early_stop_counter = 0\n",
    "    best_model_path = './best_graphormer_model.pth'\n",
    "    columns = ['epoch', 'train_loss', 'train_acc', 'train_f1', 'val_loss', 'val_acc', 'val_f1', 'test_loss', 'test_acc', 'test_f1']\n",
    "    results = []\n",
    "\n",
    "    # Initial evaluation before training\n",
    "    initial_train_loss, initial_train_acc, initial_train_f1 = evaluate(model, train_loader, device, criterion)\n",
    "    initial_val_loss, initial_val_acc, initial_val_f1 = evaluate(model, val_loader, device, criterion)\n",
    "    initial_test_loss, initial_test_acc, initial_test_f1 = evaluate(model, test_loader, device, criterion)\n",
    "\n",
    "    # Log initial metrics before the first epoch\n",
    "    results.append([0, initial_train_loss, initial_train_acc, initial_train_f1, initial_val_loss, initial_val_acc, initial_val_f1, initial_test_loss, initial_test_acc, initial_test_f1])\n",
    "\n",
    "    print(f\"Initial Metrics - Train Loss: {initial_train_loss:.4f}, Val Loss: {initial_val_loss:.4f}, Test Loss: {initial_test_loss:.4f}\")\n",
    "\n",
    "    # Save initial results to CSV before training\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df.to_csv('graphormer_training_results.csv', index=False)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch}/{num_epochs}'):\n",
    "            input_nodes = batch['input_nodes'].to(device)\n",
    "            attn_bias = batch['attn_bias'].to(device)\n",
    "            in_degree = batch['in_degree'].to(device)\n",
    "            out_degree = batch['out_degree'].to(device)\n",
    "            spatial_pos = batch['spatial_pos'].to(device)\n",
    "            attn_edge_type = batch['attn_edge_type'].to(device)\n",
    "            input_edges = batch['input_edges'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                input_nodes=input_nodes,\n",
    "                input_edges=input_edges,\n",
    "                attn_bias=attn_bias,\n",
    "                in_degree=in_degree,\n",
    "                out_degree=out_degree,\n",
    "                spatial_pos=spatial_pos,\n",
    "                attn_edge_type=attn_edge_type\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        train_acc, train_f1 = compute_metrics(torch.tensor(all_preds), torch.tensor(all_labels))\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, val_loader, device, criterion)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with validation accuracy: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"Epochs without improvement: {early_stop_counter}\")\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Test evaluation\n",
    "        test_loss, test_acc, test_f1 = evaluate(model, test_loader, device, criterion)\n",
    "\n",
    "        # Log results\n",
    "        results.append([epoch, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, test_loss, test_acc, test_f1])\n",
    "\n",
    "        # Save results to CSV after each epoch\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "        df.to_csv('graphormer_training_results.csv', index=False)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete. Best model saved as best_graphormer_model.pth\")\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed(42)\n",
    "\n",
    "# Directories containing the .npy files\n",
    "# hamiltonian_dir = './hamiltonian_small_mat'\n",
    "# non_hamiltonian_dir = './non_hamiltonian_small_mat'\n",
    "hamiltonian_dir = './hamiltonian_small_mat'\n",
    "non_hamiltonian_dir = './non_hamiltonian_small_mat'\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = GraphDataset(hamiltonian_dir, non_hamiltonian_dir)\n",
    "indices = list(range(len(dataset)))\n",
    "\n",
    "# Specify the train_val size and test size\n",
    "train_val_size = 100  # Example value, modify as per your need\n",
    "test_size = 500\n",
    "\n",
    "# Split dataset into train_val and test sets using train_test_split\n",
    "train_val_indices, test_indices = train_test_split(indices, test_size=test_size, stratify=[dataset[i]['label'] for i in indices], random_state=41)\n",
    "\n",
    "# Now, split train_val_indices into train and validation sets (80% train, 20% validation)\n",
    "train_indices, val_indices = train_test_split(train_val_indices[:train_val_size], test_size=0.2, stratify=[dataset[i]['label'] for i in train_val_indices[:train_val_size]], random_state=41)\n",
    "\n",
    "# Create subsets for train, validation, and test datasets\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "# Calculate the distribution of training, validation, and test sets\n",
    "train_counter = Counter([dataset[i]['label'].item() for i in train_indices])\n",
    "val_counter = Counter([dataset[i]['label'].item() for i in val_indices])\n",
    "test_counter = Counter([dataset[i]['label'].item() for i in test_indices])\n",
    "\n",
    "# Print the class distribution\n",
    "print(f\"Training Dataset: {train_counter[1]} True, {train_counter[0]} False\")\n",
    "print(f\"Validation Dataset: {val_counter[1]} True, {val_counter[0]} False\")\n",
    "print(f\"Test Dataset: {test_counter[1]} True, {test_counter[0]} False\")\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Define the model configuration\n",
    "max_nodes_in_dataset = max(adj.shape[0] for adj in dataset.graphs)\n",
    "# Define a smaller model configuration\n",
    "config = GraphormerConfig(\n",
    "    num_classes=2,\n",
    "    num_node_types=1,\n",
    "    num_node_features=1,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=4,\n",
    "    max_nodes=max_nodes_in_dataset,\n",
    "    multi_hop_max_dist=20,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphormerForGraphClassification(config)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, test_loader, device, optimizer, criterion, num_epochs=100, patience=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vn_gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
